{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3YhG3lCsKNfN41Qd7Uev2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaYuetong/MaYuetong/blob/main/%E2%80%9Crfbayes_ipynb%E2%80%9DALE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyW8pD9Dv_HF",
        "outputId": "87e592e7-86f1-48ed-c61f-20d07b45f61d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2518k  100 2518k    0     0  13.5M      0 --:--:-- --:--:-- --:--:-- 13.5M\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/scikit-optimize/scikit-optimize.git\n",
            "  Cloning https://github.com/scikit-optimize/scikit-optimize.git to /tmp/pip-req-build-eikagb1k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/scikit-optimize/scikit-optimize.git /tmp/pip-req-build-eikagb1k\n",
            "  Resolved https://github.com/scikit-optimize/scikit-optimize.git to commit a2369ddbc332d16d8ff173b12404b03fea472492\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.9.0) (1.2.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize==0.9.0)\n",
            "  Using cached pyaml-23.5.8-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.9.0) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.9.0) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.9.0) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize==0.9.0) (6.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->scikit-optimize==0.9.0) (3.1.0)\n",
            "Building wheels for collected packages: scikit-optimize\n",
            "  Building wheel for scikit-optimize (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-optimize: filename=scikit_optimize-0.9.0-py2.py3-none-any.whl size=100250 sha256=abefceb25328c3ae66423c0dcf3c6ff825da541a358acf9aeacee36678761e96\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-srxfoc9m/wheels/2f/f0/ed/db529a96372d05bd34f6c3a2fa7c08ef7a8314315ac46e49d7\n",
            "Successfully built scikit-optimize\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-23.5.8 scikit-optimize-0.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting shap\n",
            "  Downloading shap-0.41.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (572 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.6/572.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.65.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.1)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.41.0 slicer-0.0.7\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
        "!pip install git+https://github.com/scikit-optimize/scikit-optimize.git\n",
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#使用Random forest、神经网络、GBDT、支持向量机和Xgboost对14个自变量预测6个类型变量的代码示例。\n",
        "\n",
        "# X为14个自变量，y为6个类型变量\n",
        "data1 = pd.read_csv(\"/content/drive/MyDrive/data/DR_CZ.csv\")\n",
        "data2 = pd.read_csv(\"/content/drive/MyDrive/data/DR_NC.csv\")\n",
        "data3 = pd.read_csv(\"/content/drive/MyDrive/data/DR_QT.csv\")\n",
        "\n",
        "# ata_all = [data1, data2, data3]\n",
        "data_all = [data1]\n",
        "\n",
        "data = pd.concat(data_all, axis=0)  # axis按列合并\n",
        "# 2.切分数据输入：特征 输出：预测目标变量\n",
        "y = data.Type #因变量1个\n",
        "X = data.drop(['Dntl','Rntl','Type','wind916','wind917','wind918','prec916','prec917','prec918'], axis=1).select_dtypes(exclude=['object'])\n",
        "#自变量14个（一个自变量为一个特征：feature）,\n",
        "\n",
        "# 将标签二值化，以便绘制ROC曲线\n",
        "y = label_binarize(y, classes=[0, 1, 2, 3, 4, 5])\n",
        "n_classes = y.shape[1]\n",
        "\n",
        "# 划分训练集和测试集\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "#显示训练集测试集长度\n",
        "print(\"Train data length:\", len(X_train))\n",
        "print(\"Test data length:\", len(X_test))\n",
        "print(\"finished！\")\n",
        "\n",
        "# 定义模型\n",
        "rf = RandomForestClassifier()\n",
        "gbdt = GradientBoostingClassifier()\n",
        "svc = SVC()\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "#它使用贝叶斯优化调整超参数来训练模型，并进行交叉验证并绘制图表。\n",
        "# 定义超参数搜索空间\n",
        "rf_search_space = {\n",
        "    'n_estimators': (10, 1000),\n",
        "    'max_depth': (1, 50),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 10)\n",
        "}\n",
        "\n",
        "gbdt_search_space = {\n",
        "    'n_estimators': (10, 1000),\n",
        "    'learning_rate': (0.01, 1.0),\n",
        "    'max_depth': (1, 50),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 10)\n",
        "}\n",
        "\n",
        "\n",
        "# 使用贝叶斯优化调整超参数\n",
        "rf_bayes_search = BayesSearchCV(rf, rf_search_space)\n",
        "\n",
        "# 输出最佳超参数\n",
        "\n",
        "rf_bayes_search.fit(X,y)\n",
        "print('Random Forest best params: ', rf_bayes_search.best_params_)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7PmQVpHwoqZ",
        "outputId": "e84281c9-1275-4651-93e2-cc7650e6cf43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data length: 69991\n",
            "Test data length: 17498\n",
            "finished！\n",
            "Random Forest best params:  OrderedDict([('max_depth', 1), ('min_samples_leaf', 10), ('min_samples_split', 8), ('n_estimators', 902)])\n"
          ]
        }
      ]
    }
  ]
}